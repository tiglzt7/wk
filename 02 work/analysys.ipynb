{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "import pandas as pd\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "\n",
    "data = kw.getdf_xlsx()['sheet1']\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプルが小さい場合、サンプルを増やす"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ランダムな日付の生成\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 開始日と終了日の設定\n",
    "start_date_str = '20150101'  # 開始日（文字列形式）\n",
    "end_date = datetime.now()  # 終了日（現在の日付）\n",
    "\n",
    "# 文字列形式から日付形式に変換\n",
    "start_date = datetime.strptime(start_date_str, '%Y%m%d')\n",
    "\n",
    "# ランダムな日付を生成\n",
    "n_dates = 100\n",
    "random_dates = [start_date + (end_date - start_date) * random.random() for _ in range(n_dates)]\n",
    "random_dates = sorted(random_dates)  # 日付をソートする\n",
    "\n",
    "# 元のデータと同じ形式に日付を変換\n",
    "random_dates = [date.strftime('%Y%m%d') for date in random_dates]\n",
    "\n",
    "random_dates[:5]  # 最初の5つの日付を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with the new TESTDATEs and test_no\n",
    "new_data = pd.DataFrame([(date, test_no) for date in random_dates for test_no in range(1, 7)], \n",
    "                        columns=['TESTDATE', 'test_no'])\n",
    "\n",
    "# Show the first few rows of the new data\n",
    "new_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the new data with the original data\n",
    "data = pd.concat([data, new_data], ignore_index=True)\n",
    "\n",
    "# TESTDATEを日付形式に変換\n",
    "data['TESTDATE'] = pd.to_datetime(data['TESTDATE'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lookup table from the data for TESTDATE = 2015-09-11\n",
    "lookup_table = data[data['TESTDATE'] == '2015-09-11'].set_index('test_no')['capacity']\n",
    "\n",
    "# Fill NaNs in the 'capacity' column using the lookup table\n",
    "data.loc[data['capacity'].isna(), 'capacity'] = data['test_no'].map(lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# List of columns to fill\n",
    "columns_to_fill = ['total_head', 'current', 'output', 'shaft_power']\n",
    "\n",
    "# Standard deviation of the noise\n",
    "noise_std = 0.01\n",
    "\n",
    "# List of unique TESTDATEs\n",
    "test_dates = data['TESTDATE'].unique()\n",
    "\n",
    "# Create the lookup table for '2015-09-11'\n",
    "lookup_table_base = data[(data['TESTDATE'] == '2015-09-11')].set_index('test_no')\n",
    "\n",
    "for column in columns_to_fill:\n",
    "    for test_date in test_dates:\n",
    "        # Add noise to the lookup table values\n",
    "        lookup_table = lookup_table_base[column].copy()\n",
    "        noise = np.random.normal(0, noise_std, size=len(lookup_table))\n",
    "        lookup_table += noise\n",
    "\n",
    "        # Fill NaNs in the current column using the lookup table\n",
    "        data.loc[(data['TESTDATE'] == test_date) & data[column].isna(), column] = data.loc[(data['TESTDATE'] == test_date) & data[column].isna(), 'test_no'].map(lookup_table)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しっかりしたデータの場合はここから"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['theoretical_power'] = data['capacity'] * data['total_head'] * 9.8 / 60\n",
    "data['pump_eff'] = data['theoretical_power'] / data['shaft_power'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style and size of the plot\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(8, 10))\n",
    "\n",
    "# List of variables to plot against capacity\n",
    "variables = ['total_head', 'current', 'output', 'shaft_power', 'pump_eff']\n",
    "\n",
    "# Create a subplot for each variable\n",
    "for i, variable in enumerate(variables):\n",
    "    plt.subplot(len(variables), 1, i+1)\n",
    "    \n",
    "    # Create a line plot for each test date\n",
    "    for test_date in data['TESTDATE'].unique():\n",
    "        subset = data[data['TESTDATE'] == test_date]\n",
    "        # Convert the test date to string for labeling\n",
    "        # sns.lineplot(x='capacity', y=variable, data=subset, label=str(test_date))\n",
    "        sns.lineplot(x='capacity', y=variable, data=subset)\n",
    "        \n",
    "    # Add a title to the subplot\n",
    "    plt.title(f'Change in {variable} with respect to capacity')\n",
    "    # plt.legend(loc='best', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features\n",
    "features = ['total_head', 'current', 'output', 'shaft_power', 'pump_eff']  # replace with your actual feature names\n",
    "\n",
    "# For each feature\n",
    "for feature in features:\n",
    "    \n",
    "    # Set up the figure and axes for the 4 plots\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "    # Histogram\n",
    "    sns.histplot(data, x=feature, hue=\"test_no\", multiple=\"stack\", kde=True, ax=ax[0])\n",
    "    ax[0].set_title(f'Histogram of {feature} by test_no')\n",
    "\n",
    "    # Box plot\n",
    "    sns.boxplot(x=\"test_no\", y=feature, data=data, ax=ax[1])\n",
    "    ax[1].set_title(f'Box plot of {feature} by test_no')\n",
    "\n",
    "    # Scatter plot\n",
    "    sns.scatterplot(x=\"capacity\", y=feature, hue=\"test_no\", data=data, ax=ax[2])\n",
    "    ax[2].set_title(f'Scatter plot of {feature} vs capacity by test_no')\n",
    "\n",
    "    # Time series plot\n",
    "    sns.lineplot(x=\"TESTDATE\", y=feature, hue=\"test_no\", data=data.sort_values(['TESTDATE', 'test_no']), ax=ax[3])\n",
    "    ax[3].set_title(f'Time series plot of {feature} by test_no')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    summary = data.groupby('test_no')[features].describe()\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, feature in enumerate(features):\n",
    "    summary = data.groupby('test_no')[feature].describe()\n",
    "    # ファイルに書き出し\n",
    "    summary.to_csv(f'summary_{i}.csv')\n",
    "\n",
    "# それぞれのファイルを読み込んで表示\n",
    "for i in range(len(features)):\n",
    "    print(pd.read_csv(f'summary_{i}.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# For each feature, calculate skewness and kurtosis using scipy.stats\n",
    "for feature in features:\n",
    "    skewness = stats.skew(data[feature])\n",
    "    kurtosis = stats.kurtosis(data[feature])\n",
    "    print(f\"Feature: {feature}, Skewness: {skewness:.2f}, Kurtosis: {kurtosis:.2f}\")\n",
    "\n",
    "# Perform regression analysis using statsmodels\n",
    "X = data[['total_head', 'current', 'shaft_power']]\n",
    "Y = data['output']\n",
    "X = sm.add_constant(X)  # Add a constant column to the predictors\n",
    "\n",
    "model = sm.OLS(Y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# Print out the regression results\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"test_no\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ペアプロット\n",
    "# Pairplot for each test_no\n",
    "for test_no in data['test_no'].unique():\n",
    "    sns.pairplot(data[data['test_no'] == test_no][features])\n",
    "    plt.title(f'Pairplot for test_no {test_no}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相関ヒートマップ\n",
    "# Correlation heatmap for each test_no\n",
    "for test_no in data['test_no'].unique():\n",
    "    corr = data[data['test_no'] == test_no][features].corr()\n",
    "    \n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title(f'Correlation heatmap for test_no {test_no}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 統計的手法\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Calculate the z-score for each feature in each test_no\n",
    "for test_no in data['test_no'].unique():\n",
    "    data.loc[data['test_no'] == test_no, features] = data[data['test_no'] == test_no][features].apply(zscore)\n",
    "    \n",
    "# Mark outliers as True (absolute z-score greater than 3)\n",
    "data['outlier'] = data[features].apply(lambda x: (abs(x) > 3).any(), axis=1)\n",
    "\n",
    "# Display the first few rows of the updated data\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "# For each test_no\n",
    "for test_no in data['test_no'].unique():\n",
    "\n",
    "    # Select the data for this test_no\n",
    "    data_test = data[data['test_no'] == test_no][features]\n",
    "    outliers_test = data[data['test_no'] == test_no]['outlier']\n",
    "    \n",
    "    # Scale the data\n",
    "    data_scaled = scaler.fit_transform(data_test)\n",
    "    \n",
    "    # Fit the PCA model and transform the data\n",
    "    data_pca = pca.fit_transform(data_scaled)\n",
    "    \n",
    "    # Calculate the z-score for each point\n",
    "    z_scores = np.abs(zscore(data_test)).max(axis=1)  # We only need the maximum z-score across features\n",
    "    \n",
    "    # Plot the transformed data, coloring by z-score and marking outliers\n",
    "    plt.scatter(data_pca[:, 0], data_pca[:, 1], c=z_scores, marker='o', cmap='viridis')\n",
    "    plt.scatter(data_pca[outliers_test, 0], data_pca[outliers_test, 1], c='red', marker='x')\n",
    "    plt.title(f'PCA plot for test_no {test_no}')\n",
    "    plt.xlabel('First principal component')\n",
    "    plt.ylabel('Second principal component')\n",
    "    plt.colorbar(label='Maximum z-score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Initialize a KMeans model\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "\n",
    "# Remove rows with NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# Try the clustering again\n",
    "for test_no in data['test_no'].unique():\n",
    "\n",
    "    # Select the data for this test_no\n",
    "    data_test = data[data['test_no'] == test_no][features]\n",
    "    \n",
    "    # Scale the data\n",
    "    data_scaled = scaler.fit_transform(data_test)\n",
    "    \n",
    "    # Fit the KMeans model\n",
    "    kmeans.fit(data_scaled)\n",
    "    \n",
    "    # Calculate the distance from each point to its cluster center\n",
    "    distances = kmeans.transform(data_scaled).min(axis=1)\n",
    "    \n",
    "    # Calculate the threshold (3 standard deviations from the mean distance)\n",
    "    threshold = distances.mean() + 3*distances.std()\n",
    "    \n",
    "    # Mark outliers in the original dataframe\n",
    "    data.loc[data['test_no'] == test_no, 'outlier'] = distances > threshold\n",
    "\n",
    "# Display the first few rows of the updated data\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each test_no\n",
    "for test_no in data['test_no'].unique():\n",
    "\n",
    "    # Select the data for this test_no\n",
    "    data_test = data[data['test_no'] == test_no][features]\n",
    "    outliers_test = data[data['test_no'] == test_no]['outlier']\n",
    "    \n",
    "    # Scale the data\n",
    "    data_scaled = scaler.fit_transform(data_test)\n",
    "    \n",
    "    # Fit the PCA model and transform the data\n",
    "    data_pca = pca.fit_transform(data_scaled)\n",
    "    \n",
    "    # Plot the transformed data, coloring by cluster and marking outliers\n",
    "    plt.scatter(data_pca[:, 0], data_pca[:, 1], c=kmeans.predict(data_scaled), marker='o', cmap='viridis')\n",
    "    plt.scatter(data_pca[outliers_test, 0], data_pca[outliers_test, 1], c='red', marker='x')\n",
    "    plt.title(f'PCA plot for test_no {test_no}')\n",
    "    plt.xlabel('First principal component')\n",
    "    plt.ylabel('Second principal component')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each test_no\n",
    "for test_no in data['test_no'].unique():\n",
    "\n",
    "    # Select the data for this test_no\n",
    "    data_test = data[data['test_no'] == test_no][features]\n",
    "    \n",
    "    # Scale the data\n",
    "    data_scaled = scaler.fit_transform(data_test)\n",
    "    \n",
    "    # Fit the PCA model\n",
    "    pca.fit(data_scaled)\n",
    "    \n",
    "    # Print the explained variance ratio of the principal components\n",
    "    print(f'Explained variance ratio for test_no {test_no}: {pca.explained_variance_ratio_}')\n",
    "    \n",
    "    # Print the loadings of the principal components\n",
    "    print(f'Loadings for test_no {test_no}:')\n",
    "    loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'], index=features)\n",
    "    print(loadings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Initialize a NearestNeighbors model\n",
    "nn = NearestNeighbors(n_neighbors=4)  # The first nearest neighbor is the point itself\n",
    "\n",
    "# For each test_no\n",
    "for test_no in data['test_no'].unique():\n",
    "\n",
    "    # Select the data for this test_no\n",
    "    data_test = data[data['test_no'] == test_no][features]\n",
    "    \n",
    "    # Scale the data\n",
    "    data_scaled = scaler.fit_transform(data_test)\n",
    "    \n",
    "    # Fit the NearestNeighbors model\n",
    "    nn.fit(data_scaled)\n",
    "    \n",
    "    # Calculate the distance to the third nearest neighbor\n",
    "    distances, _ = nn.kneighbors(data_scaled)\n",
    "    distances = distances[:, -1]  # We only need the distance to the third nearest neighbor\n",
    "    \n",
    "    # Calculate the threshold (3 standard deviations from the mean distance)\n",
    "    threshold = distances.mean() + 3*distances.std()\n",
    "    \n",
    "    # Mark outliers in the original dataframe\n",
    "    data.loc[data['test_no'] == test_no, 'outlier'] = distances > threshold\n",
    "\n",
    "# Display the first few rows of the updated data\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each test_no\n",
    "for test_no in data['test_no'].unique():\n",
    "\n",
    "    # Select the data for this test_no\n",
    "    data_test = data[data['test_no'] == test_no][features]\n",
    "    outliers_test = data[data['test_no'] == test_no]['outlier']\n",
    "    \n",
    "    # Scale the data\n",
    "    data_scaled = scaler.fit_transform(data_test)\n",
    "    \n",
    "    # Fit the PCA model and transform the data\n",
    "    data_pca = pca.fit_transform(data_scaled)\n",
    "    \n",
    "    # Calculate the distance to the third nearest neighbor\n",
    "    distances, _ = nn.kneighbors(data_scaled)\n",
    "    distances = distances[:, -1]  # We only need the distance to the third nearest neighbor\n",
    "    \n",
    "    # Plot the transformed data, coloring by distance and marking outliers\n",
    "    plt.scatter(data_pca[:, 0], data_pca[:, 1], c=distances, marker='o', cmap='viridis')\n",
    "    plt.scatter(data_pca[outliers_test, 0], data_pca[outliers_test, 1], c='red', marker='x')\n",
    "    plt.title(f'PCA plot for test_no {test_no}')\n",
    "    plt.xlabel('First principal component')\n",
    "    plt.ylabel('Second principal component')\n",
    "    plt.colorbar(label='Distance to third nearest neighbor')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw.writedf_xlsx({'sheet1': data_describe})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
